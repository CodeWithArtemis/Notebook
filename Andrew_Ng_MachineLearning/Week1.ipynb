{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e719e7-ad80-43c0-88ff-5059fa66fc64",
   "metadata": {},
   "source": [
    "# Week 1 - Intro to Machine Learning\n",
    "**Learning Objectives:**\n",
    "- Define machine learning\n",
    "- Define supervised learning\n",
    "- Define unsupervised learning\n",
    "- Write and run Python code in Jupyter Notebooks\n",
    "- Define a regression model\n",
    "- Implement and visualize a cost function\n",
    "- Implement gradient descent\n",
    "- Optimize a regression model using gradient descent\n",
    "\n",
    "---\n",
    "## Ch - 1 Overview of Machine Learning\n",
    "\n",
    "### Welcome to Machine Learning\n",
    "> Machine learning is the science of getting computers to learn without being explicitly programmed ~ *Andrew Ng*\n",
    "\n",
    "Machine learning is used to find and learn patterns from the data and predict outcomes. It is sub-field of **Artificial Intelligence**(AI).\n",
    "\n",
    "Example:\n",
    "- Ranking webpages on Search engine (Google)\n",
    "- Image recognition (insta, snapchat)\n",
    "- Recommendation system (YT, Netflix)\n",
    "- Detecting spam emails\n",
    "- Weather Forecasting\n",
    "- House price prediction\n",
    "- etc...\n",
    "\n",
    "\n",
    "### Applications of Machine Learning\n",
    "Machine Learning can be use in many industries like agriculture, health-care, e-commerce etc...\n",
    "\n",
    "**Artificial General Intelligence**(AGI):\n",
    "> Looking even further into the future, many people, including me, are excited about the AI dream of someday building machines as intelligent as you or me. This is sometimes called Artificial General Intelligence or AGI. ~ *Andrew Ng*\n",
    "\n",
    "> Artificial general Intelligence is the ability of an intelligent agent to understand or learn any intellectual task that a human being can. ~ [*Wikipedia*](https://en.wikipedia.org/wiki/Artificial_general_intelligence#:~:text=Artificial%20general%20intelligence%20(AGI)%20is%20the%20ability%20of%20an%20intelligent%20agent%20to%20understand%20or%20learn%20any%20intellectual%20task%20that%20a%20human%20being%20can.)\n",
    "\n",
    "AGI is making machines intelligent as humans are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c98e5-3586-4d76-a6bc-f21a9d217130",
   "metadata": {},
   "source": [
    "# Week 1 - Intro to Machine Learning\n",
    "\n",
    "## Ch - 2 Supervised vs Unsupervised Machine learning\n",
    "\n",
    "### What is Machine Learning ?\n",
    "> Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed. ~ *Arthur Samuel*\n",
    "\n",
    "There are 3 main types of Machine learning\n",
    "1. Supervised learning\n",
    "2. Unsupervised learning\n",
    "3. Reinforcement learning\n",
    "\n",
    "<img src=\"./images/types-of-ml.png\" width=\"800px\" alt=\"types-of-ml\">\n",
    "\n",
    "[üîó](https://in.mathworks.com/discovery/reinforcement-learning/_jcr_content/mainParsys3/discoverysubsection/mainParsys/image.adapt.full.medium.png/1647932642644.png)\n",
    "\n",
    "\n",
    "- **Some more are:**\n",
    "1. Recommendation system learning\n",
    "2. Hybrid learning problems\n",
    "3. Statistical Inference\n",
    "4. Learning Techniques\n",
    "\n",
    "- **Machine learning mastery** - [Types of learning in ML](https://machinelearningmastery.com/types-of-learning-in-machine-learning/)\n",
    "\n",
    "\n",
    "### Superivsed learning\n",
    "> It refers to algorithms that learn `x` to `y` or **input** to **output** mappings.\n",
    "> \n",
    "> The key characterstic of superivsed learning is that you give your learning algorithm examples to learn from, which includes the right answers (`y`) for a given input (`x`), and is by seeing correct pairs of input `x` and desired output label `y` that the learning algorithm eventually learns to take just the input alone without the output label adn gives a reasonably accurate prediction or guess the output. ~ *Andrew Ng*\n",
    "\n",
    "In Supervised learning, the algorithm maps the input data with output labeled data and guess the output on new input based on the mapping.\n",
    "\n",
    "Example:\n",
    "|      Application       |   Input  `X`    | Output `Y`  |\n",
    "| :--------------------: | :-------------: | :---------: |\n",
    "|  Email spam detection  |     Emails      | spam or not |\n",
    "|  Image Classification  | Images (mixed)  |  Cat / Dog  |\n",
    "|    Self-driving car    | Images, sensors | move or not |\n",
    "| House price prediction |  Area, floors   |    Price    |\n",
    "\n",
    "#### Regression algorithm\n",
    "> In Regression algorithm, we try to predict a number from infinitely many possible numbers such as house prices, which could be `123,000`, `280,000` etc... ~ *Andrew Ng*\n",
    "\n",
    "We use **Regression** algorithm to predict quantitative (continuous) data, data in numbers.\n",
    "\n",
    "![regression_img](./images/regression.png)\n",
    "\n",
    "Example:\n",
    "1. Predicting house prices\n",
    "2. Bitcoin value\n",
    "3. Deaths in COVID etc...\n",
    "\n",
    "#### Classification algorithm\n",
    "> In Classification algorithm, the learning algorithm has to make a prediction of a category, all of a small set of possible outputs. Example: Cat or Dog ~ *Andrew Ng*\n",
    "\n",
    "We use **Classification** algorithm to predict qualitative (categories/classes) data, data having different categories.\n",
    "- In classification, we define boundaries by drawing lines which divide the data into different classes.\n",
    "\n",
    "<img src=\"./images/classification.png\" width=\"500px\" alt=\"classification\">\n",
    "\n",
    "Example:\n",
    "1. Breast Cancer (Yes / No) (Malignant / bengin)\n",
    "2. Image recognition (Cat or Dog)\n",
    "\n",
    "\n",
    "|           Application           | Regression | Classification |\n",
    "| :-----------------------------: | :--------: | :------------: |\n",
    "|         Price of Stock          |     ‚úÖ     |       ‚ùå      |\n",
    "| Stock will Increase or Decrease |     ‚ùå     |       ‚úÖ      |\n",
    "|        Species of Plant         |     ‚ùå     |       ‚úÖ      |\n",
    "|        Fake news or not         |     ‚ùå     |       ‚úÖ      |\n",
    "|      Temperature on Sunday      |     ‚úÖ     |       ‚ùå      |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Unsupervised learning\n",
    "> In unsupervised learning, data only comes with inputs (`x`), but not output labels (`y`). Algorithm has to find some **structure** or some **pattern** in the data.\n",
    "> \n",
    "> In Unsupervised learning, we input data (`x`) that isn't associated with any output labels (`y`), instead, our job is to find some structure or some pattern or just find something interesting in the data. ~ *Andrew Ng*\n",
    "\n",
    "In **Unsupervised learning**, we use the algorithms to identify patterns in unlabelled data, i.e. we didn't label whether the provided images is of **cat** or **dog**, algorithm has to identify patterns itself.\n",
    "\n",
    "<img src=\"./images/unsupervised.jpg\" width=\"500px\" alt=\"unsupervised\">\n",
    "\n",
    "Example:\n",
    "- Mixed and unlabelled pictures of Cats and Dogs\n",
    "- Google news\n",
    "- Finding spam emails from mixed emails.\n",
    "- Recommendation system (YT, Netflix)\n",
    "\n",
    "#### Clustering algorithm\n",
    "> Group similar data points together\n",
    "> \n",
    "> An unsupervised algorithm, might decide that the data can be assigned into two different groups or two different `clusters`. This algorithm is known as **Clustering** algorithm. ~ *Andrew Ng*\n",
    "\n",
    "Clustering means grouping the similar data together into clusters.\n",
    "\n",
    "\n",
    "<img src=\"./images/clustering.jpg\" width=\"500px\" alt=\"clustering\">\n",
    "\n",
    "\n",
    "#### Types of Unsupervised learning\n",
    "1. Anomaly detection\n",
    "\n",
    "<img src=\"./images/anomaly-house-price.png\" width=\"500px\" alt=\"anomaly-house-price\">\n",
    "\n",
    "- It is used to detect **unusual events**.\n",
    "- Example: \n",
    "Fraud detection in finanical system, where unusual events, unusual transactions could be signs of fraud.\n",
    "\n",
    "2. Dimensionality reduction\n",
    "\n",
    "<img src=\"./images/dimensionality-reduction.jpg\" width=\"500px\" alt=\"dimensionality-reduction\">\n",
    "\n",
    "[üîó](https://miro.medium.com/max/1400/1*kK4aMPHQ89ssFEus6RT4Yw.jpeg)\n",
    "\n",
    "- It takes a big dataset and magically compress it to a much smaller dataset while losing as little information as possible.\n",
    "\n",
    "\n",
    "| Application                      | Supervised | Unsupervised |\n",
    "|:--------------------------------:|:----------:|:------------:|\n",
    "| Weather Forecasting              |     ‚úÖ     |      ‚ùå     |\n",
    "| Spam Detection                   |     ‚úÖ     |      ‚ùå     |\n",
    "| Customer types                   |     ‚ùå     |      ‚úÖ     |\n",
    "| SEO (search engine optimization) |     ‚úÖ     |      ‚ùå     |\n",
    "| Medical Imaging                  |     ‚ùå     |      ‚úÖ     |\n",
    "| Classification problem           |     ‚úÖ     |      ‚ùå     |\n",
    "| YouTube Recommendation system    |     ‚ùå     |      ‚úÖ     |\n",
    "| Google news                      |     ‚ùå     |      ‚úÖ     |\n",
    "| No. of child born in next year   |     ‚úÖ     |      ‚ùå     |\n",
    "| Unkown objects in Space          |     ‚ùå     |      ‚úÖ     |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Jupyter Lab [optional] [üîó](./../codes/W1%20-%20L1%20-%20Jupyter%20Notebooks%20%5Boptional%5D.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "### Quizzes\n",
    "- Try to attempt these questions yourself.\n",
    "- And you can also submit your answers to me by creating a pull request and adding your answer in [this](quizzes/w1.md) readme file and I will review it for sure.\n",
    "\n",
    "#### Video Quiz 1\n",
    "<img src=\"./../quizzes/Video%20quiz%201%20-%20supervised%20vs%20unsupervised%20learning.jpg\" alt=\"video-quiz-1\" height=\"300px\">\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>video quiz 1</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 2nd option (<i>Would have made it worse</i>) then you are right!<br/>In general, the more opportunities you give a learning algorithm to learn, the better it will perform.</p>\n",
    "</details>\n",
    "\n",
    "#### Video Quiz 2\n",
    "<img src=\"./../quizzes/Video%20quiz%202%20-%20supervised%20vs%20unsupervised%20learning.jpg\" alt=\"video-quiz-2\" height=\"300px\">\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>video quiz 2</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 1st option (<i>Spam filtering</i>) then you are right!<br/>For instance, emails labeled as \"spam\" or \"not spam\" are examples used for training a supervised learning algorithm. The trained algorithm will then be able to predict with some degree of accuracy whether an unseen email is spam or not.</p>\n",
    "</details>\n",
    "\n",
    "#### Video Quiz 3\n",
    "<img src=\"./../quizzes/Video%20quiz%203%20-%20supervised%20vs%20unsupervised%20learning.jpg\" alt=\"video-quiz-3\" height=\"500px\">\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>video quiz 3</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 1st and 3rd options then you are right!<br/>These are the types of unsupervised learning called clustering</p>\n",
    "</details>\n",
    "\n",
    "#### Practice Quiz 1\n",
    "<img src=\"./../quizzes/Quiz%20-%201%20Supervised%20vs%20unsupervised%20learning.jpg\" alt=\"practice quiz 1\" height=\"600px\">\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>Question 1</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 2nd and 3rd options then you are right!<br/>Classification predicts from among a limited set of categories (also called classes). These could be a limited set of numbers or labels such as \"cat\" or \"dog\".<br/>Regression predicts a number among potentially infinitely possible numbers.</p>\n",
    "</details>\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>Question 2</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 2nd option then you are right!<br/>Clustering groups data into groups or clusters based on how similar each item (such as a hospital patient or shopping customer) are to each other.</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "![my_score](./../quizzes/my_score.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07347c88-d1b0-4e51-a425-0714ab382303",
   "metadata": {},
   "source": [
    "# Week 1 - Intro to Machine Learning\n",
    "\n",
    "## Ch 3: Regression Model\n",
    "\n",
    "### Linear Regression model\n",
    "<img src=\"./images/linear_regression.jpg\" alt=\"linear_regression\" width=\"500px\">\n",
    "\n",
    "> Linear Regressor model means fitting a straight line to your data. ~ *Andrew Ng*\n",
    "\n",
    "> Linear Regression is a **linear model**, e.g. a model that assumes a linear relationship between the input variables (`x`) and the single output variable (`y`). ~ *Jason Brownlee* ([Machine Learning Mastery](https://machinelearningmastery.com/linear-regression-for-machine-learning/#:~:text=Linear%20regression%20is%20a%20linear%20model%2C%20e.g.%20a%20model%20that%20assumes%20a%20linear%20relationship%20between%20the%20input%20variables%20(x)%20and%20the%20single%20output%20variable%20(y).))\n",
    "\n",
    "> Linear Regression model is a particular type of supervised learning model. It's called **regression** model because it predicts numbers as the output like prices in dollars.\n",
    "> \n",
    "> Any supervised learning model that predicts a number such as `238,000` or `3.68` or `-14.6` is addressing what's called a *regression problem*. ~ *Andrew Ng*\n",
    "\n",
    "So, *Linear Regression* model is a **supervised learning** model which predicts the single output data (`y`) by fitting the input data (`x`) into a straight line in a graph.\n",
    "\n",
    "Example:\n",
    "- If we've measured the size of a house which *Param* wants to sell, and the size is `2500` square feet.\n",
    "- So now, we will mark `2500` on **x-axis**, i.e. on house size in square feets.\n",
    "- And draw a straight line from that mark till the *regression* line.\n",
    "- Now, we will draw another straight line, this time from the point where vertical line intersects with *regression* line to the **y-axis** i.e. on house price in `$1000`.\n",
    "- And mark the point where that line intersects with **y-aixs**. That point is the price of *Param*'s house i.e. \n",
    "\n",
    "<img src=\"./images/linear_regression_example.jpg\" alt=\"linear_regression_example\" width=\"800px\">\n",
    "\n",
    "----\n",
    "\n",
    "### How `linear regression` works\n",
    "\n",
    "<img src=\"./images/supervised-learning-flow.png\" alt=\"supervised-learning-flow\" height=\"500px\">\n",
    "\n",
    "> To Train the **model**, you feed the **training dataset**, both the input `features` and the `output` targets to your learning algorithm (*linear regression*). Then your supervised learning algorithm will produce this function.\n",
    "> $$f(x) = wx + b$$\n",
    "> or more precisely, this\n",
    "> $$f_{w,b}(x) = wx + b$$\n",
    "> Here, `f` in lowercase, stands for function.\n",
    "> \n",
    "> The job of this function is to take a new input `x` and output a estimate or a prediction, which is `≈∑` (y-hat). This function is called the **model**.\n",
    "> \n",
    "> `x` is the input feature in the model and output is the prediction of the model i.e. `≈∑` (y-hat). Here, `≈∑` is the estimated value of `y`. ~ *Andrew Ng*\n",
    "\n",
    "<img src=\"./images/linear_regression.jpg\" alt=\"linear_regression\" width=\"500px\">\n",
    "\n",
    "- In above image, the **Regression line** is that function.\n",
    "\n",
    "> This `linear regression` model has `1` feature, so we call it **Linear Regression with one variable**, where the one varible means that therere's a single input variable or feature `x`, i.e. the size of the house. ~ *Andrew Ng*\n",
    "> \n",
    "> Another name for a `linear model` with `one input` variable is **Univariate linear regression**, where *uni* means *one* in latin, and *variate* means *variable*. ~ *Andrew Ng*\n",
    "\n",
    "----\n",
    "\n",
    "#### Jupyter Lab [optional] [üîó](./../codes/W1%20-%20L2%20-%20Model%20Representation%20%5Boptional%5D.ipynb)\n",
    "\n",
    "----\n",
    "\n",
    "### Cost function\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 1}^{m} (≈∑^{(i)} - y^{(i)})^2$$ \n",
    "\n",
    "> The **cost function** tell us how well the model is doing. ~ *Andrew Ng*\n",
    "\n",
    "#### Why to use cost function\n",
    "To determine the error(difference) in predicted value and actual value, we use cost function.\n",
    "\n",
    "> In **linear regression** function $f_{w,b}(x) = wx + b$, the `w` and `b` are called *parameters* of the model. \n",
    "> \n",
    "> In machine learning *parameters* of the model are the *variables* you can adjust during training in order to improve the model.\n",
    "> These parameters are also known as *weights* or *coefficients*. ~ *Andrew Ng*\n",
    "\n",
    "These **weights** help us to determine the best fit of *regression line* in our **linear regression** model, so that it can predict the best value for our **target** variable.\n",
    "\n",
    "Let's see the `regression line` with different values of `w` and `b` in *linear regression* function:\n",
    "|                 | 1                    | 2                     | 3                   |\n",
    "|-----------------|:--------------------:|:---------------------:|:-------------------:|\n",
    "| **Weights**         |`w = 0` and `b = 1.5` | `w = 0.5` and `b = 0` | `w = 0` and `b = 1` |\n",
    "| **Regression line** | <img src=\"./images/weights-graph-1.png\" alt=\"weight-graph-1\" width=\"400px\"> | <img src=\"./images/weights-graph-2.png\" alt=\"weight-graph-2\" width=\"400px\"> | <img src=\"./images/weights-graph-3.png\" alt=\"weight-graph-3\" width=\"400px\"> |\n",
    "\n",
    "\n",
    "Now, let's examine the *training set*:\n",
    "\n",
    "<img src=\"./images/regression-line-example.png\" alt=\"regression-line-example\">\n",
    "\n",
    "1. In *training set*, with **linear regression** we want to choose values for the parameters `w` and `b` which can fits the **regression line** on data perfectly.\n",
    "2. The *regression line* is formed by the function $f_{w, b}$\n",
    "3. `i` is the index / specific row of *training data*.\n",
    "4. `x`<sup>`(i)`</sup> on `x-axis` is the feature i.e. *house size*.\n",
    "5. `y`<sup>`(i)`</sup> on `y-axis` is the target variable i.e. *house price*.\n",
    "6. `x` markers in the graph are `i`<sup>th</sup> training row i.e. (x<sup>(i)</sup>, y<sup>(i)</sup>).\n",
    "7. For a given input `x`<sup>`(i)`</sup> the function $f_{w, b}$ makes a predictive value for `y` i.e. `≈∑` on `y-axis` of graph.\n",
    "8. But the actual `y` value makes some difference with `≈∑` value.\n",
    "9. That difference is **ERROR** in our function $f_{w, b}$\n",
    "\n",
    "- So, to determine the best value for `w` and `b` so that it can fit the `regression line` accurately on data, we use **cost function**.\n",
    "\n",
    "----\n",
    "\n",
    "#### Formula of Cost function\n",
    "1. First, we will subtract actual value `y` from predicted value `≈∑` (the difference is called **ERROR**) and then square the **ERROR**. \n",
    "$$(≈∑ - y)^2$$\n",
    "2. We will compute this error for each training example i.e. each `i`<sup>`th`</sup> index.\n",
    "$$(≈∑^{(i)} - y^{(i)})^2$$\n",
    "3. And we will sum up the error\n",
    "$$\\sum(≈∑^{(i)} - y^{(i)})^2$$\n",
    "4. We are starting from 1st training example, where `i = 1`\n",
    "$$\\sum_{i = 1}(≈∑^{(i)} - y^{(i)})^2$$\n",
    "5. And, we will calculate it upto `m`, i.e. total number of training examples.\n",
    "$$\\sum\\limits_{i = 1}^{m}(≈∑^{(i)} - y^{(i)})^2$$\n",
    "6. To compute the cost function whose value doesn't get bigger automatically, we will take average of `m` by dividing it with `1`.\n",
    "$$\\frac{1}{2m} \\sum\\limits_{i = 1}^{m}(≈∑^{(i)} - y^{(i)})^2$$\n",
    "7. We are dividing `1` by `2m` instead of `m` so that the cost function doesn't depend upon the number of training examples, this helps us in better comparison.\n",
    "8. We will refer to this expression to $j(w, b)$ eventually, we will find out the best values for our weights `w` and `b`.\n",
    "$$j(w, b) = \\frac{1}{2m} \\sum\\limits_{i = 1}^{m}(≈∑^{(i)} - y^{(i)})^2$$\n",
    "9. And we know that, `≈∑` is the predicted value of function $f_{w, b}(x)$, so will substitute it.\n",
    "$$j(w, b) = \\frac{1}{2m} \\sum\\limits_{i = 1}^{m}(f_{w, b}(x^{(i)}) - y^{(i)})^2$$\n",
    "10. And, our **Cost Function** is ready and this cost function is known as **Squared error cost function**.\n",
    "\n",
    "> So, formula of **Squared error cost function** is:\n",
    "> $$j(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$ \n",
    "> where \n",
    "> $$f_{w,b}(x^{(i)}) = wx^{(i)} + b$$\n",
    "\n",
    "----\n",
    "#### How cost function works\n",
    "\n",
    "In our cost function $j(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2$ we need to set such values of `w` and `b` which minimizes our $j(w, b)$ function $\\min\\limits{j(w, b)}_{w, b}$, means we need to make the error as small as possible.\n",
    "\n",
    "\n",
    "- First, let's focus only on `w` parameter.\n",
    "- So, now our *linear regression* function will be:\n",
    "$$f_w(x) = wx$$\n",
    "- And, our *Squared error cost function* will be:\n",
    "$$j(w) = \\frac{1}{2m} \\sum\\limits_{i = 1}^{m} (f_{w}(x^{(i)}) - y^{(i)})^2$$ \n",
    "- We can also substitue $f_w(x^{(i)})$ with $wx^{(i)}$\n",
    "$$j(w) = \\frac{1}{2m} \\sum\\limits_{i = 1}^{m} (wx^{(i)} - y^{(i)})^2$$ \n",
    "\n",
    "\n",
    "This is our initial graph for  $f_w(x)$ function:\n",
    "\n",
    "<img src=\"./images/simple_f_function_graph.jpg\" alt=\"simple f function graph\" width=\"400px\">\n",
    "\n",
    "- On `x-axis` we have input variable `x`, and on `y-axis` we have our target variable `y`.\n",
    "\n",
    "Now, Let's see how our *linear regression* line and *cost function* changes with different values of `w`:\n",
    "\n",
    "| values       | cost function | Explanation   | linear regression line $f_w(x)$ | cost function graph $j(w)$ |\n",
    "|:------------:|:--------------|:--------------|:-------------------------------:|:--------------------------:|\n",
    "| `w = 1`      | $=\\frac{1}{2m} [0^2 + 0^2 + 0^2]$<br/>$=\\frac{0}{2m}$<br/>$=0$                               | For each value of `i`, `x` and `y` is `1`, `2` & `3`.<br/>But `w` remain constant i.e. `1`.<br/>So, for $(wx^{(i)} - y^{(i)})^2$, each value will be $(1(x) - y)^2$, and result will be `0`.            | <img src=\"./images/regression-line-w-1-cost-function.jpg\" alt=\"regression-line-1\">            | <img src=\"./images/cost-function-w-1.jpg\" alt=\"cost-function-1\"> |\n",
    "| `w = 0.5`    | $=\\frac{1}{2m} [0.25 + 1 + 2.25]$<br/>$=\\frac{0}{2m} [3.5]$<br/>$=\\frac{3.5}{6}$<br/>$={0.58}$    |  For each value of `i`, `x` and `y` is `1`, `2` & `3`.<br/>But `w` remain constant i.e. `1`.<br/>So, for $(wx^{(i)} - y^{(i)})^2$, each value will be <br/>`x=1`,<br/>$(0.5(1) - 1)^2 = (0.5 - 1)^2 = 0.25$<br/>`x=2`, <br/>$(0.5(2) - 2)^2 = (1 - 2)^2 = 1$<br/>and `x=3`,<br/>$(0.5(3) - 3)^2 = (1.5 - 3)^2 = 2.25$<br/>and result will be `0.58` approx.            | <img src=\"./images/regression-line-w-2-cost-function.jpg\" alt=\"regression-line-2\">            | <img src=\"./images/cost-function-w-2.jpg\" alt=\"cost-function-2\"> |\n",
    "| `w = 0`      | $=\\frac{1}{2m} [(1)^2 + (2)^2 + (3)^2]$<br/>$=\\frac{0}{2m} [14]$<br/>$=\\frac{14}{6}$<br/>$={2.3}$ | For each value of `i`, `x` and `y` is `1`, `2` & `3`.<br/>But `w` remain constant i.e. `1`.<br/>So, for $(wx^{(i)} - y^{(i)})^2$, each value will be <br/>`x=1`,<br/>$(0(1) - 1)^2 = (1)^2$<br/>`x=2`,<br/>$(0(2) - 2)^2=(2)^2$<br/>and `x=3`,<br/>$(0(3) - 3)^2=(3)^2$ and result will be `2.3` approx.<br/> And because `w=0`, so any value of `x` multiplied by `w` resulted `0`. <br/> Hence, our *regression lin* will be made on `x-axis`.            | <img src=\"./images/regression-line-w-3-cost-function.jpg\" alt=\"regression-line-3\">            | <img src=\"./images/cost-function-w-3.jpg\" alt=\"cost-function-3\"> |\n",
    "\n",
    "> So, we have visualized the **cost function** graph with `3` different values of `w`, but if we keep doing this, for different values of `w`. We will end up with a shape like this:\n",
    "\n",
    "<img src=\"./images/cost-function-w-final.jpg\" alt=\"cost-function-graph\" width=\"600px\">\n",
    "\n",
    "> Hence, choosing `w=1` fits the *regression line* well on our data.\n",
    "\n",
    "- Let's see how the **Cost function** graph will show with both parameters `w` and `b`.\n",
    "\n",
    "<img src=\"./images/contour-plot.jpg\" alt=\"contour-plot\" width=\"900px\">\n",
    "\n",
    "- This is how our **Cost function** graph will look.\n",
    "- This is a 3-d *contour plot*, having `3` axis, `x-axis` for `w`, `y-axis` for `b` and `z-axis` for $j(w, b)$\n",
    "\n",
    "Imagine *contour plot* like a mountain e.g. **Mount Fuji**\n",
    "\n",
    "<img src=\"./images/mount-fuji-contour.jpg\" alt=\"mount-fuji-contour\" width=\"500px\">\n",
    "\n",
    "- This is a *topographical map* of **Mount Fuji**, the horizontal <font color=\"red\">red</font> lines in this mountain are **contours**.\n",
    "- If you see this from top, you'll see all the **contours** of same *height*.\n",
    "\n",
    "<img src=\"./images/mount-fuji-contour-above.jpg\" alt=\"mount-fuji-contour-above\" width=\"500px\">\n",
    "\n",
    "- Now, if you cut all the *contours* of 3d-**contour plot** and place them on a single plane, then the 2-d**contour plot** will look like this:\n",
    "\n",
    "<img src=\"./images/contour-plot-2d.jpg\" alt=\"contour-plot-2d\" width=\"500px\">\n",
    "\n",
    "- Here, each oval / line is the contour or the $j(w,b)$ / `z-axis`.\n",
    "- And on `x-axis` we have `w` parameter, on `y-axis` we have `b` parameter.\n",
    "- So, now if we choose different `w` and `b` on same oval, then our $j(w,b)$ function will be same, means *cost function* will result the same output.\n",
    "\n",
    "<img src=\"./images/contour-full-plots.jpg\" alt=\"contour-full-plots\" width=\"800px\">\n",
    "\n",
    "Here, all these `3` markers have same *cost function* value, because they are plotted on *same height* aka same value of $j(w,b)$.\n",
    "\n",
    "- So, if we want to minimize our *cost function* here, then the smallest oval or 1st oval in cocentric ovals has minimum value.\n",
    "\n",
    "<img src=\"./images/contour-plot-2d-minimum.jpg\" alt=\"contour-plot-2d-minimum\" width=\"600px\">\n",
    "\n",
    "\n",
    "- Now, we know what **contour plot** is, and how it works, let's play with `w` and `b` parameters and finds the *regression line* that best fits the data.\n",
    "\n",
    "\n",
    "| value                         | linear regression function | graph                                                                        |\n",
    "|:-----------------------------:|:---------------------------|:----------------------------------------------------------------------------:|\n",
    "| `w = 0.15`<br/>`b = 800`      | $f(x) = 0.15x + 800$       | <img src=\"./images/cost-function-1.jpg\" alt=\"cost-function-1\" width=\"800px\"> |\n",
    "| `w = 0`<br/>`b = 360`         | $f(x) = 0x + 360$          | <img src=\"./images/cost-function-2.jpg\" alt=\"cost-function-2\" width=\"800px\"> |\n",
    "| `w = -0.15`<br/>`b = 500`     | $f(x) = -0.15x + 500$      | <img src=\"./images/cost-function-3.jpg\" alt=\"cost-function-3\" width=\"800px\"> |\n",
    "| `w = 0.13`<br/>`b = 71`       | $f(x) = 0.13x + 71$        | <img src=\"./images/cost-function-4.jpg\" alt=\"cost-function-4\" width=\"800px\"> |\n",
    "\n",
    "\n",
    "- We can clearly see that with values of `w = 0.13` and `b = 71`, our **Squared error cost function** is minimized, means have less error. So, we can use these values for `w` and `b` parameters in our *linear regression* function to make our model best fit to data, and do predictions accurately.\n",
    "- There are still some *erors* within our *regression line* and *actual data*\n",
    "\n",
    "![cost-function-minimize](./images/cost-function-minimize.jpg)\n",
    "\n",
    "- But these are minimal errors that we can have.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "### Notations in Machine Learning\n",
    "\n",
    "**Note**: These are just standard names for specific data in **ML**, it is not compulsory to follow these notations.\n",
    "\n",
    "\n",
    "Example House price data:\n",
    "\n",
    "<img src=\"./images/house_price_data.jpg\" alt=\"house_price_data\" width=\"500px\">\n",
    "\n",
    "\n",
    "1. `Training data` is the data we use to train our model by which our model learns.\n",
    "2. We use lowercase `x` to denote the input data. Example: `x` is house size.\n",
    "3. We use lowercase `y` to denote the output variable / target variable. Example `y` is house price.\n",
    "4. We use lowercase `m` for total number of training data. Example: `m=47`.\n",
    "5. We use lowercase `x` & `y` in parenthesis or in tuple like `(x, y)` to denote a single training data. Example `(x, y) = (2500, 410)`.\n",
    "6. To refer a specific training data, we use `x` & `y` same as above in parenthesis or a tuple with **superscripted** `i` to both of them, where `i` is the index to a specific row like (x<sup>i</sup>, y<sup>i</sup>). Example: (x<sup>i</sup>, y<sup>i</sup>) = (1534<sup>3</sup>, 315<sup>3</sup>).\n",
    "\n",
    "\n",
    "| Notation                                 | Meaning                               | Example                                       |\n",
    "|-----------------------------------------:|:--------------------------------------|:---------------------------------------------:|\n",
    "| Training data                            | data which our model learns           | house price data                              |\n",
    "| `x`                                      | input data                            | house size                                    |\n",
    "| `y`                                      | output / target variable              | house price                                   |\n",
    "| `m`                                      | total number of training data         | `47`                                          |\n",
    "| `(x, y)`                                 | single training data                  | `(2500, 410)`                                 |\n",
    "| `i`                                      | index / row of training data          | `3`                                           |\n",
    "| (x<sup>i</sup>, y<sup>i</sup>)           | `i`<sup>th</sup> training data        | (1534<sup>3</sup>, 315<sup>3</sup>)           |\n",
    "| $f_{w,b}(x) = wx + b$                    | linear regression equation            | -                                             |\n",
    "| `w, b`                                   | weights of linear regression equation | -                                             |\n",
    "| $j(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2$ | Squared Error Cost function | -                 |\n",
    "\n",
    "----\n",
    "\n",
    "#### Jupyter Lab [optional] [üîó](./../codes/W1%20-%20L3%20-%20Cost%20function%20%5Boptional%5D.ipynb)\n",
    "\n",
    "----\n",
    "\n",
    "### Quizzes\n",
    "\n",
    "#### Video quiz 1\n",
    "<img src=\"./../quizzes/Video%20quiz%204%20-%20regression%20model.jpg\" alt=\"video-quiz-1\" width=\"60%\">\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>video quiz 1</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 2nd option then you are right!<br/>y is the true value for that training example, referred to as the output variable, or ‚Äútarget‚Äù.</p>\n",
    "</details>\n",
    "\n",
    "#### Video quiz 2\n",
    "<img src=\"./../quizzes/Video%20quiz%205%20-%20regression%20model.jpg\" alt=\"video-quiz-2\" width=\"60%\">\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>video quiz 2</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 1st option then you are right!<br/>w and b are parameters of the model, adjusted as the model learns from the data. They‚Äôre also referred to as ‚Äúcoefficients‚Äù or ‚Äúweights‚Äù.</p>\n",
    "</details>\n",
    "\n",
    "#### Video quiz 3\n",
    "<img src=\"./../quizzes/Video%20quiz%206%20-%20regression%20model.jpg\" alt=\"video-quiz-3\" width=\"60%\">\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>video quiz 3</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 2nd option (When the cost J is at or near a minimum) then you are right!<br/>When the cost is relatively small, closer to zero, it means the model fits the data better compared to other choices for w and b.</p>\n",
    "</details>\n",
    "\n",
    "#### Practice quiz 1\n",
    "<img src=\"./../quizzes/Quiz%20-%202%20Regression%20model.jpg\" alt=\"practice-quiz-1\" width=\"60%\">\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>Question 1</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 4th option (x) then you are right!<br/>The xx, the input features, are fed into the model to generate a prediction fw,b(x)</p>\n",
    "</details>\n",
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='#00FF00'>Answer to <b>Question 2</b></font>\n",
    "</summary>\n",
    "    <p>If you have selected 2nd option (The selected values of the parameters ww and bb cause the algorithm to fit the training set really well.) then you are right!<br/>When the cost is small, this means that the model fits the training set well.</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cfb1f-b2dd-4b23-967c-9510fc0ef980",
   "metadata": {},
   "source": [
    "# Week 1 - Intro to Machine Learning\n",
    "\n",
    "## Ch 4: Train the model with gradient descent\n",
    "\n",
    "### Gradient Descent\n",
    "#### Definition\n",
    "> Gradient Descent is an algorithm that you can apply to try to minimize the cost function of the models ~ *Andrew Ng*\n",
    "\n",
    "> Gradient Descent is an optimization algorithm used to find the values of paramter (*coefficients*) of a function (`f`) that minimizes a cost function (*cost*). ~ *Jason Brownlee* ([ML Mastery](https://machinelearningmastery.com/gradient-descent-for-machine-learning/#:~:text=Gradient%20descent%20is%20an%20optimization%20algorithm%20used%20to%20find%20the%20values%20of%20parameters%20(coefficients)%20of%20a%20function%20(f)%20that%20minimizes%20a%20cost%20function%20(cost).))\n",
    "\n",
    "#### Explanation\n",
    "> Gradient Descent can be applied to more general *cost* functions that work with models that have more than `2` parameters.\n",
    ">\n",
    "> For instance, if you have a cost function $J(w1, w2, w3,...b)$, your objective is to minimize `j` over the parameters `w1`, `w2` to `wn` and `b`. In other words, you want to pick vlaues for `w1` through `wn` and `b` that gives you the smalles possible value of `j`. ~ *Andrew Ng*\n",
    "\n",
    "#### Working\n",
    "> To try to minimize this cost function $j(w, b)$ i.e. $\\min j(w,b)$, we will start with some initial guesses for `w` and `b`. In *linear regression*, it won't matter what the initial values are, so we can set `w=0` and `b=0`. With gradient descent algorithm, we'll keep on changing the parameters `w` and `b` a bit every time to try to reduce the *cost* $j(w, b)$, until `j` settles at or near minimum. ~ *Andrew Ng*\n",
    "\n",
    "#### Example:\n",
    "Let's look at this cost function graph.\n",
    "\n",
    "**Note**: This is not a *linear regression* graph, rather it is graph of cost function of *neural networks*. We are just using it as an example for better learning.\n",
    "\n",
    "<img src=\"./images/gradient-descent-graph-1.jpg\" alt=\"gradient-descent-graph-1\" width=\"800px\">\n",
    "\n",
    "1. Imagine, you're on a top of hill, and you want to go to the one of the bottom valley's as efficiently as possible.\n",
    "\n",
    "<img src=\"./images/gradient-descent-top-1.jpg\" alt=\"gradient-descent-graph-1\" width=\"800px\">\n",
    "\n",
    "2. What *gradient descent* does it, you spin around `360` degrees and look around and finds the tiny baby step in one direction that takes you down the hill as quickly as possible.\n",
    "3. Then, you go steep downhill in that direction. Mathematically, the direction of *steepest descent*.\n",
    "4. Now, again after going a little bit down, you again spin `360` degrees and find small step which takes you down. And you'll go there.\n",
    "5. You'll do this again and again until you reach the bottom of the hill.\n",
    "\n",
    "<img src=\"./images/gradient-descent-graph-2.jpg\" alt=\"gradient-descent-graph-3\" width=\"800px\">\n",
    "\n",
    "\n",
    "This is an interesting property of *gradient descent*, that it automatically finds the efficient way to go to opposite direction (in this case, bottom of hill) from the starting values for the parameters `w` and `b` which you've chosed.\n",
    "\n",
    "Let's try once more:\n",
    "\n",
    "1. This time you'll choose a different starting point by choosing different values for parameters `w` and `b`.\n",
    "2. If you repeat the *gradient descent* process you'll end up here, totally different valley (downside).\n",
    "\n",
    "<img src=\"./images/gradient-descent-graph-3.jpg\" alt=\"gradient-descent-graph-3\" width=\"800px\">\n",
    "\n",
    "\n",
    "- These both bottom minimum points are called **local minima**.\n",
    "\n",
    "> But for *linear regression*, the *Squared error cost function* doesn't and will never have multiple *local minima*.\n",
    "> \n",
    "> It has a single **global minima** because of it's *bowl-shape*.\n",
    "> *Squared error cost function* also known as ***Convex function***.\n",
    "> \n",
    "> This is how graph of **Convex function** looks like:\n",
    "> \n",
    "> <img src=\"./images/linear-regression-cost-function-graph.jpg\" alt=\"convex function graph\" width=\"50%\" style=\"min-width:800px\">\n",
    "> \n",
    "> One nice property of **convex function** is that, as long as you're choosing *learning rate* properly, it will always converge to the **global minimum**. ~ *Andrew Ng*\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "#### Algorithm\n",
    "> $$repeat\\enspace until\\enspace convergence \\{ $$\n",
    "> $$w = w - Œ± \\frac{dj(w,b)}{dw}$$\n",
    "> $$b = b - Œ± \\frac{dj(w,b)}{db}$$\n",
    "> $$ \\} $$\n",
    "\n",
    "This is the algorithm of **Gradient Descent**, where we update both the *coefficients* `w` and `b` simultaneously.\n",
    "\n",
    "And, while updating the *coefficients*, we need to make sure that we should use old values in the formula, and assign new values after calculating new values for both *coefficients*.\n",
    "\n",
    "| Right way ‚úÖ | Wrong way ‚ùå |\n",
    "|:------------:|:-------------:|\n",
    "| $new\\_w = w - Œ± \\frac{dj(w,b)}{dw}$<br/>$new\\_b = b - Œ± \\frac{dj(w,b)}{db}$<br/>$w = new\\_w$<br/>$b = new\\_b$           | $w = w - Œ± \\frac{dj(w,b)}{dw}$<br/>$b = b - Œ± \\frac{d}{db} j(w,b)$ |\n",
    "\n",
    "> Œ± (alpha) is the *learning rate*. The learning rate is a small positive number b/w `0` and `1`. Alpha decides how big a step is. If alpha is very large like `0.9`, it corresponds to very aggressive gradient descent procedure. If alpha is very small like `0.01`, it means you're taking very small steps to find the *local minima*.\n",
    "> \n",
    "> $\\frac{dj(w,b)}{dw}$ and $\\frac{dj(w,b)}{db}$ are the *derivative* or more precisely *partial derivative*. We use *derivative* to find whether should we increase or decrease the *coefficient*. ~ *Andrew Ng*\n",
    "\n",
    "\n",
    "####  Working of Gradient Descent Algorithm\n",
    "\n",
    "#### How *derivative* effects *coefficients*\n",
    "1. Let's say we have our cost function $j(w)$ with only one parameter `w`.\n",
    "$$j(w) = \\frac{1}{2m}\\sum\\limits^m_{i=1}(f_w(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "And it's graph looks something like this:\n",
    "\n",
    "<img src=\"./images/derivative-1.jpg\" alt=\"j function graph\" width=\"500px\">\n",
    "\n",
    "2. And we have choose `w` (marked point) on the graph.\n",
    "3. First, we will draw a tangent which touches that point.\n",
    "4. And we will make a triangle like this:\n",
    "\n",
    "<img src=\"./images/derivative-2.jpg\" alt=\"tangent for derivate graph\" width=\"500px\">\n",
    "\n",
    "5. Now, we will find the *slope* of this tangent.\n",
    "6. If, the slope is positive, i.e. $\\frac{d}{dw} > 0$, then our new value of `w` parameter will be smaller than it's old value.\n",
    "7. Because, we are multiplying the *slope* with *learning rate* and subtracting it from the actual value of `w`.\n",
    "8. The new `w` will be smaller and hence closer to the minimum possible value.\n",
    "9. And we want the minimum value of `w`\n",
    "\n",
    "<img src=\"./images/derivative-3.jpg\" alt=\"w is getting closer to minimum value\" width=\"500px\">\n",
    "\n",
    "- Let's take another example:\n",
    "\n",
    "<img src=\"./images/derivative-4.jpg\" alt=\"j function graph for another example of derivative\" width=\"500px\">\n",
    "\n",
    "1. If we choose a value for `w` on left hand side on graph.\n",
    "2. We'll draw it's tangent like this:\n",
    "\n",
    "<img src=\"./images/derivative-5.jpg\" alt=\"tangent for derivate graph\" width=\"500px\">\n",
    "\n",
    "3. It's *slope* will be negative, because we want the minimum possible value for `w`.\n",
    "4. So, new value of `w` will be bigger than it's acutal value.\n",
    "5. Because, we are multiplying the *slope* with *learning rate* and subtracting it from the actual value of `w` and subtracting a negative number means adding a positive number. (10 - (-8) == 10 + (8))\n",
    "6. So, at the ending our value of `w` will increase.\n",
    "\n",
    "<img src=\"./images/derivative-6.jpg\" alt=\"final derivate graph\" width=\"500px\">\n",
    "\n",
    "\n",
    "#### How *learning rate* effects *coefficients*\n",
    "1. Let's say we choose a small learning rate like `0.0001`.\n",
    "2. And our $j(w)$ function graph looks like this:\n",
    "\n",
    "<img src=\"./images/learning-rate-graph-1.jpg\" alt=\"learning rate graph\" width=\"500px\">\n",
    "\n",
    "3. And we choose the value of `w` parameter on the marker.\n",
    "4. And we draw the tangent and find it's *slope* and multiply it by *learning rate*.\n",
    "5. So, lower the learning rate, means taking a very small step.\n",
    "6. Small steps takes a lot of time, to reach the minimum value of `w`.\n",
    "7. So, *gradient descent* will work very slow.\n",
    "\n",
    "<img src=\"./images/learning-rate-graph-2.jpg\" alt=\"learning graph reaching minimum value\" width=\"500px\">\n",
    "\n",
    "Let's see what happens, if we choose a large learning rate like `0.9`.\n",
    "\n",
    "1. At first our $j(w)$ function graph look like this:\n",
    "\n",
    "<img src=\"./images/learning-rate-graph-3.jpg\" alt=\"learning rate graph another example\" width=\"500px\">\n",
    "\n",
    "2. And we choose the value of `w` parameter closer to the minimum value.\n",
    "3. If we draw the tangent and find it's *slope* an multiply it by *learning rate*.\n",
    "4. We, eventually skips the minimum value and goes far from minimum value of `w`.\n",
    "5. Because higher *learning rate* lead to take bigger step.\n",
    "6. And we never reach the minimum value, rather go away from it.\n",
    "\n",
    "<img src=\"./images/learning-rate-graph-4.jpg\" alt=\"learning rate graph another example\" width=\"500px\">\n",
    "\n",
    "> #### If Œ± is too large...\n",
    "> Gradient descent may:\n",
    "> - Overshoot, never reach minimum\n",
    "> - Fail to converge, diverge    ~ *Andrew Ng*\n",
    "\n",
    "Now, what if `w` parameter is already at local minima, but there are other local minima points too, which are away from it:\n",
    "\n",
    "<img src=\"./images/learning-rate-graph-5.jpg\" alt=\"learning rate graph\" width=\"500px\">\n",
    "\n",
    "1. Now, if we draw the tangent on that point and find it's slope.\n",
    "2. It will be `0`.\n",
    "3. So, multiplying *learning rate* with *derivative* `0` will be `0`.\n",
    "4. Means `w` parameter will be same as it is.\n",
    "$$w = w - Œ± \\frac{dj(w)}{dw}$$\n",
    "$$w = w - Œ± 0$$\n",
    "$$w = w - 0$$\n",
    "$$w = w$$\n",
    "\n",
    "5. So, at the end it doesn't change, if the or *derivative* i.e. *slope* is 0.\n",
    "6. Because, it is already at local minimum value.\n",
    "\n",
    "<img src=\"./images/learning-rate-graph-6.jpg\" alt=\"learning rate graph tangent\" width=\"500px\">\n",
    "\n",
    " - Hence, this means we can reach local minimum with fixed learning rate.\n",
    "\n",
    "Means, let's say initial value of `w` is this:\n",
    "\n",
    "<img src=\"./images/learning-rate-graph-7.jpg\" alt=\"fixed learning rate graph\" width=\"500px\">\n",
    "\n",
    "1. We draw a tangent, find it's *slope* and go down.\n",
    "2. Again we repeat it, but this time, it is less steeper then previous.\n",
    "3. So, after repeating it for multiple times with a fixed *learning rate*, we will reach at local minimum.\n",
    "\n",
    "<img src=\"./images/learning-rate-graph-8.jpg\" alt=\"fixed learning rate graph local minimum\" width=\"500px\">\n",
    "\n",
    "> #### near a learning rate:\n",
    "> - *Derivative* becomes smaller\n",
    "> - Update steps becomes smaller\n",
    "\n",
    "#### How derivatives $\\frac{dj(w,b)}{dw}$ and $\\frac{d}{dwb}j(w,b)$ are calculated ? [optional]\n",
    "> It turns out if calculate these derivatives with respect to `w` and `b` with below formulae, it will work.\n",
    ">\n",
    "> For `w` *derivative*:\n",
    "> $$\\frac{dj(w,b)}{dw} = \\frac{1}{m}\\sum\\limits_{i=1}^m(f_{w,b}(x^{(i)} - y^{(i)})x^{(i)}$$\n",
    "> For `b` *derivative*:\n",
    "> $$\\frac{dj(w,b)}{dw} = \\frac{1}{m}\\sum\\limits_{i=1}^m(f_{w,b}(x^{(i)} - y^{(i)})$$\n",
    "> These formulae are derived from *calculus*. ~ *Andrew Ng*\n",
    "\n",
    "#### How formula $\\frac{1}{m}\\sum\\limits_{i=1}^m(f_{w,b}(x^{(i)} - y^{(i)})x^{(i)}$ for `w` parameter derives:\n",
    "\n",
    "**Note**: I don't know *calculus*, but I have derived it accordingly how *Andrew Ng* sir said.\n",
    "\n",
    "We know that:\n",
    "$$\\frac{dj(w,b)}{dw}\\enspace=\\enspace\\frac{d}{dw}\\frac{1}{2m}\\sum\\limits_{i=1}^m(f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "We can also write it as:\n",
    "$$\\frac{dj(w,b)}{dw}\\enspace=\\enspace\\frac{d}{dw}\\frac{1}{2m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})^2$$\n",
    "\n",
    "By rules of *calculus* this formula is equal to this:\n",
    "$$\\frac{d}{dw}\\frac{1}{2m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})^2\\enspace=\\enspace\\frac{1}{2m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})2x^{(i)}$$\n",
    "\n",
    "And we cancel out `2`:\n",
    "$$\\frac{1}{\\cancel{2}m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})\\cancel{2}x^{(i)}$$\n",
    "\n",
    "After cancelling out `2`, we got this final formula for `w` parameter:\n",
    "> $$\\frac{1}{m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})x^{(i)}$$\n",
    "\n",
    "#### How formula $\\frac{dj(w,b)}{dw} = \\frac{1}{m}\\sum\\limits_{i=1}^m(f_{w,b}(x^{(i)} - y^{(i)})$ for `b` parameter derives:\n",
    "\n",
    "Similarly, we know that:\n",
    "$$\\frac{dj(w,b)}{db}\\enspace=\\enspace\\frac{d}{db}\\frac{1}{2m}\\sum\\limits_{i=1}^m(f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "We can also write it as:\n",
    "$$\\frac{dj(w,b)}{db}\\enspace=\\enspace\\frac{d}{db}\\frac{1}{2m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})^2$$\n",
    "\n",
    "By rules of *calculus* this formula is equal to this:\n",
    "$$\\frac{d}{db}\\frac{1}{2m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})^2\\enspace=\\enspace\\frac{1}{2m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})2$$\n",
    "\n",
    "And we cancel out `2`:\n",
    "$$\\frac{1}{\\cancel{2}m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})\\cancel{2}$$\n",
    "\n",
    "After cancelling out `2`, we got this final formula for `b` parameter:\n",
    "> $$\\frac{1}{m}\\sum\\limits_{i=1}^m(wx^{(i)}+b - y^{(i)})$$\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### Gradient Descent in Action\n",
    "\n",
    "| Iterations | Cost function value | graph                                                                                               |\n",
    "|:----------:|:-------------------:|:---------------------------------------------------------------------------------------------------:|\n",
    "| 1.         | `77,237`            | <img src=\"./images/gradient-descent-action-1.jpg\" alt=\"gradient descent iteration 1\" width=\"750px\"> |\n",
    "| 2.         | `45,401`            | <img src=\"./images/gradient-descent-action-2.jpg\" alt=\"gradient descent iteration 2\" width=\"750px\"> |\n",
    "| 3.         | `29,109`            | <img src=\"./images/gradient-descent-action-3.jpg\" alt=\"gradient descent iteration 3\" width=\"750px\"> |\n",
    "| 4.         | `19,076`            | <img src=\"./images/gradient-descent-action-4.jpg\" alt=\"gradient descent iteration 4\" width=\"750px\"> |\n",
    "| 5.         | `13,318`            | <img src=\"./images/gradient-descent-action-5.jpg\" alt=\"gradient descent iteration 5\" width=\"750px\"> |\n",
    "| 6.         | `9,460`             | <img src=\"./images/gradient-descent-action-6.jpg\" alt=\"gradient descent iteration 6\" width=\"750px\"> |\n",
    "| 7.         | `5,729`             | <img src=\"./images/gradient-descent-action-7.jpg\" alt=\"gradient descent iteration 7\" width=\"750px\"> |\n",
    "| 8.         | `3,559`             | <img src=\"./images/gradient-descent-action-8.jpg\" alt=\"gradient descent iteration 8\" width=\"750px\"> |\n",
    "| 9.         | `2,311`             | <img src=\"./images/gradient-descent-action-9.jpg\" alt=\"gradient descent iteration 9\" width=\"750px\"> |\n",
    "\n",
    "So, at the end, we got our *regression line* which fits our data well after `9` iterations.\n",
    "\n",
    "> This type of *Gradient Descent* is known as ***Batch Gradient Descent***. Here, *Batch* means we are looking at all training examples.\n",
    "> \n",
    "> There are other versions of *Gradient Descent* also, which doesn't use all training examples, but instead looks at smaller subsets of training data at each update step. ~ *Andrew Ng*\n",
    "\n",
    "\n",
    "---- \n",
    "\n",
    "### Jupyter lab [optional] [üîó](../codes/W1-%20L4%20-%20Gradient%20Descent%20[optional].ipynb)\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "### Quizzes\n",
    "\n",
    "#### Video quiz 1\n",
    "\n",
    "<img src=\"../quizzes/Video%20quiz%207%20-%20gradient%20descent.jpg\" alt=\"video quiz 1\" width=\"60%\">\n",
    "<details>\n",
    "<summary>    \n",
    "    <font size='3', color='#00FF00'>Answer to <b>video quiz 1</b></font>\n",
    "</summary>\n",
    "<p>If you have selected option b (Updates parameter w by a small amount) then you are right! This updates the parameter by a small amount, in order to reduce the cost J.</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "#### Video quiz 2\n",
    "\n",
    "<img src=\"../quizzes/Video%20quiz%208%20-%20gradient%20descent.jpg\" alt=\"video quiz 2\" width=\"60%\">\n",
    "<details>\n",
    "<summary>    \n",
    "    <font size='3' color='#00FF00'>Answer to <b>video quiz 2</b></font>\n",
    "</summary>\n",
    "<p>If you have selected option d (w decreases) then you are right! The learning rate Œ± is always a positive number, so if you take W minus a positive number, you end up with a new value for W that is smaller.</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "#### Practice quiz 1\n",
    "\n",
    "<img src=\"../quizzes/Quiz%20-%203%20Gradient%20Descent.jpg\" alt=\"Practice quiz 1\" width=\"60%\">\n",
    "<details>\n",
    "<summary>    \n",
    "    <font size='3' color='#00FF00'>Answer to <b>question 1</b></font>\n",
    "</summary>\n",
    "<p>If you have selected option a (w increases) then you are right! The learning rate is always a positive number, so if you take W minus a negative number, you end up with a new value for W that is larger (more positive).</p>\n",
    "</details>\n",
    "<details>\n",
    "<summary>    \n",
    "    <font size='3' color='#00FF00'>Answer to <b>question 2</b></font>\n",
    "</summary>\n",
    "<p>If you have selected option b then you are right!</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf00b31-8ae3-4299-b7bd-570d6f820974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
